{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_corr.corr(method='spearman')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(df_corr.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(df_corr.columns)\n",
    "ax.set_yticklabels(df_corr.columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_dist_plots(dataset):\n",
    "    n_bins = 20\n",
    "\n",
    "    for cluster in dataset['Cluster_ID'].unique():\n",
    "        display(\"Distribution for cluster {}\".format(cluster))\n",
    "\n",
    "        # create subplots\n",
    "        fig, ax = plt.subplots(nrows=5)\n",
    "\n",
    "        ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "\n",
    "        plt.figure(figsize = (6,22))\n",
    "        for j, col in enumerate(numeric_cols):\n",
    "            # create the bins\n",
    "            bins = np.linspace(min(dataset[col]), max(dataset[col]), 20)\n",
    "            # plot distribution of the cluster using histogram\n",
    "            sns.distplot(dataset[dataset['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True)\n",
    "            ax[j].set_ylabel(f'{\" \" * int((1.7* len(col))) }{col}', rotation=30 )#.set_rotation(0)\n",
    "            ax[j].yaxis.set_label_position('right')\n",
    "            ax[j].set_xlabel('')\n",
    "            #ax[j].yaxis.rotation=30\n",
    "            #ax[j].xaxis.set_label_position('top')\n",
    "            # plot the normal distribution with a black line\n",
    "\n",
    "            sns.distplot(dataset[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "            ax[j].set_xlabel('')\n",
    "\n",
    "        ##plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validity(model, Y_train, X_train, model_name):\n",
    "    Y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    residuals = Y_train_pred - Y_train\n",
    "        \n",
    "    fig = plt.figure(figsize=[20, 16])\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "    ax.set_title('Q-Q Plot for '+ model_name)\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    ax.hist(residuals, 50)\n",
    "    ax.set_title('Histogram of Residuals for '+ model_name)\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.scatter(Y_train_pred, residuals)\n",
    "    ax.set_title('Residuals for '+ model_name)\n",
    "    ax.set_xlabel('Fitted')\n",
    "    ax.set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# plot each col onto one ax\n",
    "for col, ax in zip(data.columns, axes.flat):\n",
    "    data[col].plot.bar(ax=ax, rot=0)\n",
    "    ax.set_title(col)\n",
    "    \n",
    "# disable leftover axes\n",
    "for ax in axes.flat[data.columns.size:]:\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_of_rows = 5\n",
    "nbr_of_cols = 2\n",
    " \n",
    "coords = [(r, c) for r in range(nbr_of_rows) for c in range(nbr_of_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nbr_of_rows, nbr_of_cols, figsize=(12,12))\n",
    " \n",
    "for i, yr in enumerate(range(2010, 2020)):\n",
    "    r,c = coords[i]  # grab the pre-built coordinates\n",
    "    d = df[df.Year==yr][['Name', 'Gross']].sort_values('Gross').tail(10)\n",
    "    _ = ax[r][c].barh(d.Name, d.Gross)\n",
    "    _ = ax[r][c].set_title('Top 10 grossing movies in {0}'.format(yr))\n",
    "     \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(model, history):\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    ax.plot(history.history['loss'], label = 'Overall Loss')\n",
    "    ax.plot(history.history['segmentation_loss'], label = 'Segmentation Loss')\n",
    "    ax.plot(history.history['classification_loss'], label = 'Classification Loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    #ax.plot(history.history['accuracy'], label = 'Overall Accuracy')\n",
    "    ax.plot(history.history['segmentation_accuracy'], label = 'Segmentation Accuracy')\n",
    "    ax.plot(history.history['classification_accuracy'], label = 'Classification Accuracy')\n",
    "    ax.legend()\n",
    "    ax.set_title('Training Accuracy')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.plot(history.history['val_loss'], label = 'Overall Loss')\n",
    "    ax.plot(history.history['val_segmentation_loss'], label = 'Segmentation Loss')\n",
    "    ax.plot(history.history['val_classification_loss'], label = 'Classification Loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('Testing Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    #ax.plot(history.history['val_accuracy'], label = 'Overall Accuracy')\n",
    "    ax.plot(history.history['val_segmentation_accuracy'], label = 'Segmentation Accuracy')\n",
    "    ax.plot(history.history['val_classification_accuracy'], label = 'Classification Accuracy')\n",
    "    ax.legend()\n",
    "    ax.set_title('Testing Accuracy')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model, x_train_image, y_train_class, x_test_image, y_test_class, model_name, train_time, process_time_df, perf_results_df):\n",
    "    fig = plt.figure(figsize=[14, 8])    \n",
    "    \n",
    "    # predict on the training set\n",
    "    train_pred_start = process_time()\n",
    "    train_class_pred, train_seg_pred = model.predict(x_train_image, verbose=False)\n",
    "    train_pred_end = process_time()\n",
    "\n",
    "    # get indexes for the predictions and ground truth \n",
    "    indexes = tf.argmax(train_class_pred, axis=1)\n",
    "    #gt_idx = tf.argmax(y_train_class, axis=1)\n",
    "    train_f1_class = round(f1_score(y_train_class, indexes, average='weighted'), 4)\n",
    "\n",
    "    # plot the confusion matrix -train set\n",
    "    ax = fig.add_subplot(1, 2, 1) \n",
    "    confusion_mtx = tf.math.confusion_matrix(y_train_class, indexes) \n",
    "    sns.heatmap(confusion_mtx, xticklabels=range(37), yticklabels=range(37), \n",
    "            annot=True, fmt='g', ax=ax, annot_kws={\"fontsize\":8}, cbar=False)\n",
    "    ax.set_title('Training, F1 Score: %f' % train_f1_class)\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('Actual label')\n",
    "\n",
    "    # predict on the test set\n",
    "    test_pred_start = process_time()\n",
    "    test_class_pred, test_seg_pred = model.predict(x_test_image, verbose=False)\n",
    "    test_pred_end = process_time()\n",
    "    \n",
    "    indexes = tf.argmax(test_class_pred, axis=1) # predicted\n",
    "    #gt_idx = tf.argmax(y_test_class, axis=1)\n",
    "    test_f1_class = round(f1_score(y_test_class, indexes, average='weighted'), 4)\n",
    "    \n",
    "    # plot the confusion matrix - test set\n",
    "    ax = fig.add_subplot(1, 2, 2) \n",
    "    confusion_mtx = tf.math.confusion_matrix(y_test_class, indexes) \n",
    "    sns.heatmap(confusion_mtx, xticklabels=range(37), yticklabels=range(37), \n",
    "            annot=True, fmt='g', ax=ax, annot_kws={\"fontsize\":8}, cbar=False)\n",
    "    ax.set_title('Testing, F1 Score: %f' % test_f1_class)\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('Actual label')\n",
    "\n",
    "    # Collect inference times\n",
    "    train_inference_time = round(train_pred_end-train_pred_start, 4)\n",
    "    test_inference_time = round(test_pred_end-test_pred_start, 4)\n",
    "    process_time_df[model_name]= [train_time, train_inference_time, test_inference_time]\n",
    "\n",
    "    # Collect F1 Scores\n",
    "    perf_results_df[model_name] = [train_f1_class, test_f1_class]\n",
    "\n",
    "    display(process_time_df)\n",
    "    display(perf_results_df)\n",
    "    \n",
    "    print(classification_report(y_test_class, indexes, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    fig = plt.figure(figsize=[12, 3])\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    conf = ConfusionMatrixDisplay.from_estimator(model, X_train, Y_train, normalize=None, xticks_rotation='horizontal', ax=ax, colorbar=False)\n",
    "    pred = model.predict(X_train)\n",
    "    conf.ax_.set_title('Training Performance: F1 score ' + str(round(f1_score(Y_train, model.predict(X_train), average='weighted'), 3)));\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    conf = ConfusionMatrixDisplay.from_estimator(model, X_val, Y_val, normalize=None, xticks_rotation='horizontal', ax=ax,colorbar=False)\n",
    "    pred = model.predict(X_val)\n",
    "    conf.ax_.set_title('Validation Performance: F1 score ' + str(round(f1_score(Y_val, model.predict(X_val), average='weighted'), 3)));\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    conf = ConfusionMatrixDisplay.from_estimator(model, X_test, Y_test, normalize=None, xticks_rotation='horizontal', ax=ax,colorbar=False)\n",
    "    pred = model.predict(X_test)\n",
    "    conf.ax_.set_title(f\"Testing Performance: F1 score {str(round(f1_score(Y_test, model.predict(X_test), average='weighted'), 3))}\");\n",
    "\n",
    "    print(classification_report(Y_val, model.predict(X_val), digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250, max_depth=15, random_state=24)\n",
    "rf.fit(X_train, Y_train)\n",
    "eval_model(rf, X_train, Y_train, X_val, Y_val,  X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_est = [100, 200, 400, 800]\n",
    "max_depth = [None, 1, 2, 4, 8, 16]\n",
    "class_weights = [None, 'balanced', 'balanced_subsample']\n",
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "best_acc = 0\n",
    "rf_best_model = None\n",
    "\n",
    "for e in num_est:\n",
    "    for d in max_depth:\n",
    "        for w in class_weights:\n",
    "            for c in criterion: \n",
    "            \n",
    "                rf = RandomForestClassifier(n_estimators=e, max_depth=d, random_state = 42, class_weight = w, criterion = c, n_jobs=-1).fit(X_train, Y_train)\n",
    "\n",
    "                acc = f1_score(Y_val, rf.predict(X_val), average='weighted')\n",
    "                if (acc > best_acc):\n",
    "                    best_acc = acc\n",
    "                    rf_best_model = rf\n",
    "\n",
    "eval_model(rf_best_model, X_train, Y_train, X_val, Y_val,  X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "_ = tree.plot_tree(rf_best_model.estimators_[0], filled=True, fontsize=9.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import numpy\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import AveragePooling2D, Input, Flatten\n",
    "from tensorflow.keras import activations\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pydot\n",
    "import IPython\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot, plot_model\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet_layer(inputs,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "    \"\"\"2D Convolution-Batch Normalization-Activation stack builder\n",
    "\n",
    "    # Arguments\n",
    "        inputs (tensor): input tensor from input image or previous layer\n",
    "        num_filters (int): Conv2D number of filters\n",
    "        kernel_size (int): Conv2D square kernel dimensions\n",
    "        strides (int): Conv2D square stride dimensions\n",
    "        activation (string): activation name\n",
    "        batch_normalization (bool): whether to include batch normalization\n",
    "        conv_first (bool): conv-bn-activation (True) or\n",
    "            bn-activation-conv (False)\n",
    "\n",
    "    # Returns\n",
    "        x (tensor): tensor as input to the next layer\n",
    "    \"\"\"\n",
    "    conv = Conv2D(num_filters,\n",
    "                  kernel_size=kernel_size,\n",
    "                  strides=strides,\n",
    "                  padding='same',\n",
    "                  kernel_initializer='he_normal',\n",
    "                  kernel_regularizer=l2(1e-4))\n",
    "\n",
    "    x = inputs\n",
    "    if conv_first:\n",
    "        x = conv(x)\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "    else:\n",
    "        if batch_normalization:\n",
    "            x = BatchNormalization()(x)\n",
    "        if activation is not None:\n",
    "            x = Activation(activation)(x)\n",
    "        x = conv(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def resnet_v1(inputs, filters, num_res_blocks, pool_size):\n",
    "    \"\"\"ResNet Version 1 Model builder [a]\n",
    "\n",
    "    Stacks of 2 x (3 x 3) Conv2D-BN-ReLU\n",
    "    Last ReLU is after the shortcut connection.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filters is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same number of filters.\n",
    "\n",
    "    # Arguments\n",
    "        inputs (layer):         the input tensor\n",
    "        filters ([int]):        number of filters in each stage, length of list determines number of stages\n",
    "        num_res_blocks (int):   number of residual blocks per stage\n",
    "        pool_size (int):        size of the average pooling at the end\n",
    "\n",
    "    # Returns\n",
    "        output after global average pooling and flatten, ready for output\n",
    "    \"\"\"\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=filters[0])\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stack, filters in enumerate(filters):\n",
    "        for res_block in range(num_res_blocks):\n",
    "            strides = 1\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                strides = 2  # downsample\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=filters,\n",
    "                             strides=strides)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=filters,\n",
    "                             activation=None)\n",
    "            if stack > 0 and res_block == 0:  # first layer but not first stack\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=filters,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "            x = Activation('relu')(x)\n",
    " \n",
    "    # Add classifier on top.\n",
    "    # v1 does not use BN after last shortcut connection-ReLU\n",
    "    x = AveragePooling2D(pool_size=pool_size)(x)\n",
    "    y = Flatten()(x)\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "def resnet_v2(inputs, filters, num_res_blocks, pool_size):\n",
    "    \"\"\"ResNet Version 2 Model builder [b]\n",
    "\n",
    "    Stacks of (1 x 1)-(3 x 3)-(1 x 1) BN-ReLU-Conv2D or also known as\n",
    "    bottleneck layer\n",
    "    First shortcut connection per layer is 1 x 1 Conv2D.\n",
    "    Second and onwards shortcut connection is identity.\n",
    "    At the beginning of each stage, the feature map size is halved (downsampled)\n",
    "    by a convolutional layer with strides=2, while the number of filter maps is\n",
    "    doubled. Within each stage, the layers have the same number filters and the\n",
    "    same filter map sizes.\n",
    "\n",
    "    # Arguments\n",
    "        inputs (layer):         the input tensor\n",
    "        filters ([int]):        number of filters in each stage, length of list determines number of stages\n",
    "        num_res_blocks (int):   number of residual blocks per stage\n",
    "        pool_size (int):        size of the average pooling at the end\n",
    "\n",
    "    # Returns\n",
    "        output after global average pooling and flatten, ready for output\n",
    "    \"\"\"\n",
    "\n",
    "    x = resnet_layer(inputs=inputs,\n",
    "                     num_filters=filters[0],\n",
    "                     conv_first=True)\n",
    "\n",
    "    # Instantiate the stack of residual units\n",
    "    for stage, filters in enumerate(filters):\n",
    "        num_filters_in = filters\n",
    "        for res_block in range(num_res_blocks):\n",
    "            activation = 'relu'\n",
    "            batch_normalization = True\n",
    "            strides = 1\n",
    "            if stage == 0:\n",
    "                num_filters_out = num_filters_in * 4\n",
    "                if res_block == 0:  # first layer and first stage\n",
    "                    activation = None\n",
    "                    batch_normalization = False\n",
    "            else:\n",
    "                num_filters_out = num_filters_in * 2\n",
    "                if res_block == 0:  # first layer but not first stage\n",
    "                    strides = 2    # downsample\n",
    "\n",
    "            # bottleneck residual unit\n",
    "            y = resnet_layer(inputs=x,\n",
    "                             num_filters=num_filters_in,\n",
    "                             kernel_size=1,\n",
    "                             strides=strides,\n",
    "                             activation=activation,\n",
    "                             batch_normalization=batch_normalization,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_in,\n",
    "                             conv_first=False)\n",
    "            y = resnet_layer(inputs=y,\n",
    "                             num_filters=num_filters_out,\n",
    "                             kernel_size=1,\n",
    "                             conv_first=False)\n",
    "            if res_block == 0:\n",
    "                # linear projection residual shortcut connection to match\n",
    "                # changed dims\n",
    "                x = resnet_layer(inputs=x,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=None,\n",
    "                                 batch_normalization=False)\n",
    "            x = keras.layers.add([x, y])\n",
    "\n",
    "        num_filters_in = num_filters_out\n",
    "\n",
    "    # Add classifier on top.\n",
    "    # v2 has BN-ReLU before Pooling\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = AveragePooling2D(pool_size=pool_size)(x)\n",
    "    y = Flatten()(x)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1, ), name='img')\n",
    "x = resnet_v1(inputs, [16, 32], 1, 14)\n",
    "output = Dense(10)(x)\n",
    "model_resnet_v1 = keras.Model(inputs=inputs, outputs=output, name='simple_resnet_v1')\n",
    "keras.utils.plot_model(model_resnet_v1, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(28, 28, 1, ), name='img')\n",
    "x = resnet_v2(inputs, [16, 32], 1, 14)\n",
    "output = Dense(10)(x)\n",
    "model_resnet_v2 = keras.Model(inputs=inputs, outputs=output, name='simple_resnet_v2')\n",
    "keras.utils.plot_model(model_resnet_v2, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval(model, x_train, y_train, x_test, y_test, filename, batch_size, epochs):\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(filename, verbose=1, monitor='val_loss',save_best_only=True, mode='auto')\n",
    "    \n",
    "    history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        callbacks=[checkpoint])    \n",
    "    \n",
    "    model.load_weights(filename)\n",
    "    model.save(filename)\n",
    "    \n",
    "    fig = plt.figure(figsize=[30, 10])\n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    ax.legend()\n",
    "    ax.set_title('Training Performance')\n",
    "\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    pred = model.predict(x_train);\n",
    "    indexes = tf.argmax(pred, axis=1)\n",
    "    cm = confusion_matrix(y_train, indexes)\n",
    "    c = ConfusionMatrixDisplay(cm, display_labels=range(10))\n",
    "    c.plot(ax = ax)    \n",
    "    ax.set_title('Training')\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    pred = model.predict(x_test);\n",
    "    indexes = tf.argmax(pred, axis=1)\n",
    "    cm = confusion_matrix(y_test, indexes)\n",
    "    c = ConfusionMatrixDisplay(cm, display_labels=range(10))\n",
    "    c.plot(ax = ax)    \n",
    "    ax.set_title('Testing')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
