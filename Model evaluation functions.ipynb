{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df_corr.corr(method='spearman')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,len(df_corr.columns),1)\n",
    "ax.set_xticks(ticks)\n",
    "plt.xticks(rotation=90)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(df_corr.columns)\n",
    "ax.set_yticklabels(df_corr.columns)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_dist_plots(dataset):\n",
    "    n_bins = 20\n",
    "\n",
    "    for cluster in dataset['Cluster_ID'].unique():\n",
    "        display(\"Distribution for cluster {}\".format(cluster))\n",
    "\n",
    "        # create subplots\n",
    "        fig, ax = plt.subplots(nrows=5)\n",
    "\n",
    "        ax[0].set_title(\"Cluster {}\".format(cluster))\n",
    "\n",
    "        plt.figure(figsize = (6,22))\n",
    "        for j, col in enumerate(numeric_cols):\n",
    "            # create the bins\n",
    "            bins = np.linspace(min(dataset[col]), max(dataset[col]), 20)\n",
    "            # plot distribution of the cluster using histogram\n",
    "            sns.distplot(dataset[dataset['Cluster_ID'] == cluster][col], bins=bins, ax=ax[j], norm_hist=True)\n",
    "            ax[j].set_ylabel(f'{\" \" * int((1.7* len(col))) }{col}', rotation=30 )#.set_rotation(0)\n",
    "            ax[j].yaxis.set_label_position('right')\n",
    "            ax[j].set_xlabel('')\n",
    "            #ax[j].yaxis.rotation=30\n",
    "            #ax[j].xaxis.set_label_position('top')\n",
    "            # plot the normal distribution with a black line\n",
    "\n",
    "            sns.distplot(dataset[col], bins=bins, ax=ax[j], hist=False, color=\"k\")\n",
    "            ax[j].set_xlabel('')\n",
    "\n",
    "        ##plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_validity(model, Y_train, X_train, model_name):\n",
    "    Y_train_pred = model.predict(X_train)\n",
    "    \n",
    "    residuals = Y_train_pred - Y_train\n",
    "        \n",
    "    fig = plt.figure(figsize=[20, 16])\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    stats.probplot(residuals, dist=\"norm\", plot=ax)\n",
    "    ax.set_title('Q-Q Plot for '+ model_name)\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    ax.hist(residuals, 50)\n",
    "    ax.set_title('Histogram of Residuals for '+ model_name)\n",
    "\n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.scatter(Y_train_pred, residuals)\n",
    "    ax.set_title('Residuals for '+ model_name)\n",
    "    ax.set_xlabel('Fitted')\n",
    "    ax.set_ylabel('Residuals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8, 8), constrained_layout=True, sharey=True)\n",
    "\n",
    "# plot each col onto one ax\n",
    "for col, ax in zip(data.columns, axes.flat):\n",
    "    data[col].plot.bar(ax=ax, rot=0)\n",
    "    ax.set_title(col)\n",
    "    \n",
    "# disable leftover axes\n",
    "for ax in axes.flat[data.columns.size:]:\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbr_of_rows = 5\n",
    "nbr_of_cols = 2\n",
    " \n",
    "coords = [(r, c) for r in range(nbr_of_rows) for c in range(nbr_of_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nbr_of_rows, nbr_of_cols, figsize=(12,12))\n",
    " \n",
    "for i, yr in enumerate(range(2010, 2020)):\n",
    "    r,c = coords[i]  # grab the pre-built coordinates\n",
    "    d = df[df.Year==yr][['Name', 'Gross']].sort_values('Gross').tail(10)\n",
    "    _ = ax[r][c].barh(d.Name, d.Gross)\n",
    "    _ = ax[r][c].set_title('Top 10 grossing movies in {0}'.format(yr))\n",
    "     \n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cramers_v(x, y):\n",
    "    confusion_matrix = pd.crosstab(x,y)\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum().sum()\n",
    "    phi2 = chi2/n\n",
    "    r,k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r-((r-1)**2)/(n-1)\n",
    "    kcorr = k-((k-1)**2)/(n-1)\n",
    "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_models(model, history):\n",
    "    fig = plt.figure(figsize=[10, 10])\n",
    "    ax = fig.add_subplot(2, 2, 1)\n",
    "    ax.plot(history.history['loss'], label = 'Overall Loss')\n",
    "    ax.plot(history.history['segmentation_loss'], label = 'Segmentation Loss')\n",
    "    ax.plot(history.history['classification_loss'], label = 'Classification Loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('Training Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 2)\n",
    "    #ax.plot(history.history['accuracy'], label = 'Overall Accuracy')\n",
    "    ax.plot(history.history['segmentation_accuracy'], label = 'Segmentation Accuracy')\n",
    "    ax.plot(history.history['classification_accuracy'], label = 'Classification Accuracy')\n",
    "    ax.legend()\n",
    "    ax.set_title('Training Accuracy')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 3)\n",
    "    ax.plot(history.history['val_loss'], label = 'Overall Loss')\n",
    "    ax.plot(history.history['val_segmentation_loss'], label = 'Segmentation Loss')\n",
    "    ax.plot(history.history['val_classification_loss'], label = 'Classification Loss')\n",
    "    ax.legend()\n",
    "    ax.set_title('Testing Loss')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Loss')\n",
    "    \n",
    "    ax = fig.add_subplot(2, 2, 4)\n",
    "    #ax.plot(history.history['val_accuracy'], label = 'Overall Accuracy')\n",
    "    ax.plot(history.history['val_segmentation_accuracy'], label = 'Segmentation Accuracy')\n",
    "    ax.plot(history.history['val_classification_accuracy'], label = 'Classification Accuracy')\n",
    "    ax.legend()\n",
    "    ax.set_title('Testing Accuracy')\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_performance(model, x_train_image, y_train_class, x_test_image, y_test_class, model_name, train_time, process_time_df, perf_results_df):\n",
    "    fig = plt.figure(figsize=[14, 8])    \n",
    "    \n",
    "    # predict on the training set\n",
    "    train_pred_start = process_time()\n",
    "    train_class_pred, train_seg_pred = model.predict(x_train_image, verbose=False)\n",
    "    train_pred_end = process_time()\n",
    "\n",
    "    # get indexes for the predictions and ground truth \n",
    "    indexes = tf.argmax(train_class_pred, axis=1)\n",
    "    #gt_idx = tf.argmax(y_train_class, axis=1)\n",
    "    train_f1_class = round(f1_score(y_train_class, indexes, average='weighted'), 4)\n",
    "\n",
    "    # plot the confusion matrix -train set\n",
    "    ax = fig.add_subplot(1, 2, 1) \n",
    "    confusion_mtx = tf.math.confusion_matrix(y_train_class, indexes) \n",
    "    sns.heatmap(confusion_mtx, xticklabels=range(37), yticklabels=range(37), \n",
    "            annot=True, fmt='g', ax=ax, annot_kws={\"fontsize\":8}, cbar=False)\n",
    "    ax.set_title('Training, F1 Score: %f' % train_f1_class)\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('Actual label')\n",
    "\n",
    "    # predict on the test set\n",
    "    test_pred_start = process_time()\n",
    "    test_class_pred, test_seg_pred = model.predict(x_test_image, verbose=False)\n",
    "    test_pred_end = process_time()\n",
    "    \n",
    "    indexes = tf.argmax(test_class_pred, axis=1) # predicted\n",
    "    #gt_idx = tf.argmax(y_test_class, axis=1)\n",
    "    test_f1_class = round(f1_score(y_test_class, indexes, average='weighted'), 4)\n",
    "    \n",
    "    # plot the confusion matrix - test set\n",
    "    ax = fig.add_subplot(1, 2, 2) \n",
    "    confusion_mtx = tf.math.confusion_matrix(y_test_class, indexes) \n",
    "    sns.heatmap(confusion_mtx, xticklabels=range(37), yticklabels=range(37), \n",
    "            annot=True, fmt='g', ax=ax, annot_kws={\"fontsize\":8}, cbar=False)\n",
    "    ax.set_title('Testing, F1 Score: %f' % test_f1_class)\n",
    "    ax.set_xlabel('Predicted label')\n",
    "    ax.set_ylabel('Actual label')\n",
    "\n",
    "    # Collect inference times\n",
    "    train_inference_time = round(train_pred_end-train_pred_start, 4)\n",
    "    test_inference_time = round(test_pred_end-test_pred_start, 4)\n",
    "    process_time_df[model_name]= [train_time, train_inference_time, test_inference_time]\n",
    "\n",
    "    # Collect F1 Scores\n",
    "    perf_results_df[model_name] = [train_f1_class, test_f1_class]\n",
    "\n",
    "    display(process_time_df)\n",
    "    display(perf_results_df)\n",
    "    \n",
    "    print(classification_report(y_test_class, indexes, digits = 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n",
    "    fig = plt.figure(figsize=[12, 3])\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 1)\n",
    "    conf = ConfusionMatrixDisplay.from_estimator(model, X_train, Y_train, normalize=None, xticks_rotation='horizontal', ax=ax, colorbar=False)\n",
    "    pred = model.predict(X_train)\n",
    "    conf.ax_.set_title('Training Performance: F1 score ' + str(round(f1_score(Y_train, model.predict(X_train), average='weighted'), 3)));\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 2)\n",
    "    conf = ConfusionMatrixDisplay.from_estimator(model, X_val, Y_val, normalize=None, xticks_rotation='horizontal', ax=ax,colorbar=False)\n",
    "    pred = model.predict(X_val)\n",
    "    conf.ax_.set_title('Validation Performance: F1 score ' + str(round(f1_score(Y_val, model.predict(X_val), average='weighted'), 3)));\n",
    "    \n",
    "    ax = fig.add_subplot(1, 3, 3)\n",
    "    conf = ConfusionMatrixDisplay.from_estimator(model, X_test, Y_test, normalize=None, xticks_rotation='horizontal', ax=ax,colorbar=False)\n",
    "    pred = model.predict(X_test)\n",
    "    conf.ax_.set_title(f\"Testing Performance: F1 score {str(round(f1_score(Y_test, model.predict(X_test), average='weighted'), 3))}\");\n",
    "\n",
    "    print(classification_report(Y_val, model.predict(X_val), digits = 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=250, max_depth=15, random_state=24)\n",
    "rf.fit(X_train, Y_train)\n",
    "eval_model(rf, X_train, Y_train, X_val, Y_val,  X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_est = [100, 200, 400, 800]\n",
    "max_depth = [None, 1, 2, 4, 8, 16]\n",
    "class_weights = [None, 'balanced', 'balanced_subsample']\n",
    "criterion = ['gini', 'entropy', 'log_loss']\n",
    "\n",
    "best_acc = 0\n",
    "rf_best_model = None\n",
    "\n",
    "for e in num_est:\n",
    "    for d in max_depth:\n",
    "        for w in class_weights:\n",
    "            for c in criterion: \n",
    "            \n",
    "                rf = RandomForestClassifier(n_estimators=e, max_depth=d, random_state = 42, class_weight = w, criterion = c, n_jobs=-1).fit(X_train, Y_train)\n",
    "\n",
    "                acc = f1_score(Y_val, rf.predict(X_val), average='weighted')\n",
    "                if (acc > best_acc):\n",
    "                    best_acc = acc\n",
    "                    rf_best_model = rf\n",
    "\n",
    "eval_model(rf_best_model, X_train, Y_train, X_val, Y_val,  X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rf_best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5))\n",
    "_ = tree.plot_tree(rf_best_model.estimators_[0], filled=True, fontsize=9.5) "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
